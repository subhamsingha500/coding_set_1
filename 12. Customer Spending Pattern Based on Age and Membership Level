//Scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Spending_Pattern")
  .getOrCreate()
  
val customers = List(
("karthik", "Premium", 1050, 32),
("neha", "Standard", 800, 28),
("priya", "Premium", 1200, 40),
("mohan", "Basic", 300, 35),
("ajay", "Standard", 700, 25),
("vijay", "Premium", 500, 45),
("veer", "Basic", 450, 33),
("aatish", "Standard", 600, 29),
("animesh", "Premium", 1500, 60),
("nishad", "Basic", 200, 21)
).toDF("name", "membership", "spending", "age")

val workdf = customers
  .withColumn("Spender_type",
   when(col("spending")>1000 && col("membership").isin("Premium"), "High Spende")
   .when(col("spending").between(500,1000) && col("membership").isin("Standard"),"Average Spender").otherwise("Low Spender"))

workdf.show()

val agg_membership_df = workdf.groupBy("membership").agg(avg("spending"))

agg_membership_df.show()


//PySpark

%python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, avg

spark = SparkSession.builder.appName("Spending_Pattern").getOrCreate()


  
customers = [
("karthik", "Premium", 1050, 32),
("neha", "Standard", 800, 28),
("priya", "Premium", 1200, 40),
("mohan", "Basic", 300, 35),
("ajay", "Standard", 700, 25),
("vijay", "Premium", 500, 45),
("veer", "Basic", 450, 33),
("aatish", "Standard", 600, 29),
("animesh", "Premium", 1500, 60),
("nishad", "Basic", 200, 21)
]
customers_df = spark.createDataFrame(customers, ["name", "membership", "spending", "age"])

workdf = customers_df \
  .withColumn("Spender_type",
   when((col("spending")>1000) & (col("membership").isin("Premium")), "High Spender")
   .when(col("spending").between(500,1000) & col("membership").isin("Standard"),"Average Spender").otherwise("Low Spender"))

workdf.show()

agg_membership_df = workdf.groupBy("membership").agg(avg("spending"))

agg_membership_df.show()
