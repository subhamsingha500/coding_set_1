//Scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Working_shift")
  .getOrCreate()

import spark.implicits._

val employees = List(
  ("karthik", 62),
  ("neha", 50),
  ("priya", 30),
  ("mohan", 65),
  ("ajay", 40),
  ("vijay", 47),
  ("veer", 55),
  ("aatish", 30),
  ("animesh", 75),
  ("nishad", 60)
).toDF("name", "hours_worked")

val workdf = employees
  .withColumn("name", initcap(col("name")))
  .withColumn("overtime_calculation", 
    when(col("hours_worked") > 60, "Excessive Overtime")
      .when(col("hours_worked").between(45, 60), "Standard Overtime")
      .otherwise("No Overtime")
  )

workdf.show()

//PySpark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, initcap

spark = SparkSession.builder.appName("SalesPerformanceByAgent").getOrCreate()


employees = [
("karthik", 62),
("neha", 50),
("priya", 30),
("mohan", 65),
("ajay", 40),
("vijay", 47),
("veer", 55),
("aatish", 30),
("animesh", 75),
("nishad", 60)
]
employees_df = spark.createDataFrame(employees, ["name", "hours_worked"])

workdf = employees_df \
.withColumn("name", initcap(col("name"))) \
.withColumn("overtime_calculation",
    when(col("hours_worked") > 60, "Excessive Overtime") 
      .when(col("hours_worked").between(45,60) , "Standard Overtime") 
      .otherwise("No Overtime") 
  )

workdf.show()

