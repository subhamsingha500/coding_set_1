//SCALA_Spark
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.DateType

val spark = SparkSession.builder.appName("EmployeeStatusCheck").getOrCreate()

import spark.implicits._

val employees = List(
  ("karthik", "2024-11-01"),
  ("neha", "2024-10-20"),
  ("priya", "2024-10-28"),
  ("mohan", "2024-11-02"),
  ("ajay", "2024-09-15"),
  ("vijay", "2024-10-30"),
  ("veer", "2024-10-25"),
  ("aatish", "2024-10-10"),
  ("animesh", "2024-10-15"),
  ("nishad", "2024-11-01"),
  ("varun", "2024-10-05"),
  ("aadil", "2024-09-30")
).toDF("name", "last_checkin")

val employeesWithDate = employees.withColumn("last_checkin", col("last_checkin").cast(DateType))

val currentDate = current_date()
val statusCheckDF = employeesWithDate
  .withColumn("name", initcap(col("name"))) 
  .withColumn("status", when(datediff(currentDate, col("last_checkin")) <= 7, "Active").otherwise("Inactive"))

statusCheckDF.show()

//PySpark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_date, datediff, when, initcap
from pyspark.sql.types import DateType


spark = SparkSession.builder.appName("EmployeeStatusCheck").getOrCreate()
employees = [
("karthik", "2024-11-01"),
("neha", "2024-10-20"),
("priya", "2024-10-28"),
("mohan", "2024-11-02"),
("ajay", "2024-09-15"),
("vijay", "2024-10-30"),
("veer", "2024-10-25"),
("aatish", "2024-10-10"),
("animesh", "2024-10-15"),
("nishad", "2024-11-01"),
("varun", "2024-10-05"),
("aadil", "2024-09-30")
]
employees_df = spark.createDataFrame(employees, ["name", "last_checkin"]

employees_df1 = employees_df.withColumn("last_checkin", col("last_checkin").cast(DateType()))

current_date_col = current_date()
status_check_df = employees_df1
.withColumn(
    "name", initcap(col("name"))  
)
.withColumn(
    "status",
    when(datediff(current_date_col, col("last_checkin")) <= 7, "Active").otherwise("Inactive")
)

status_check_df.show()
