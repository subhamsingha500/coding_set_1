//SCALA

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder.appName("SalesPerfomance").getOrCreate()
import spark.implicits._

val sales = List(
  ("karthik", 60000),
  ("neha", 48000),
  ("priya", 30000),
  ("mohan", 24000),
  ("ajay", 52000),
  ("vijay", 45000),
  ("veer", 70000),
  ("aatish", 23000),
  ("animesh", 15000),
  ("nishad", 8000),
  ("varun", 29000),
  ("aadil", 32000)
).toDF("name", "total_sales")

val performanceDF = sales
  .withColumn("performance_status",
    when(col("total_sales") > 50000, "Excellent")
      .when(col("total_sales").between(25000, 50000), "Good")
      .otherwise("Needs Improvement")
  )
  .withColumn("name", initcap(col("name")))  

val aggregatedDF = performanceDF
  .groupBy("performance_status")
  .agg(sum("total_sales").alias("total_sales"))

aggregatedDF.show()


//PySpark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, initcap, sum 

spark = SparkSession.builder.appName("SalesPerformanceByAgent").getOrCreate()

sales = [
    ("karthik", 60000),
    ("neha", 48000),
    ("priya", 30000),
    ("mohan", 24000),
    ("ajay", 52000),
    ("vijay", 45000),
    ("veer", 70000),
    ("aatish", 23000),
    ("animesh", 15000),
    ("nishad", 8000),
    ("varun", 29000),
    ("aadil", 32000)
]
sales_df = spark.createDataFrame(sales, ["name", "total_sales"])

performance_df = sales_df.withColumn(
    "name", initcap(col("name"))  
).withColumn(
    "performance_status", when(col("total_sales") > 50000, "Excellent")
    .when((col("total_sales") >= 25000) & (col("total_sales") <= 50000), "Good").otherwise("Needs Improvement"))

aggregated_df = performance_df.groupBy("performance_status").agg(sum("total_sales").alias("total_sales"))



aggregated_df.show()

